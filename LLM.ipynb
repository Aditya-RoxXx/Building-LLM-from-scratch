{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "fb35db24-0730-47ad-a074-be9efd323706",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since raw text cannot be used directly in neural networks, the data is converted into continuous-valued vectors.\n",
    "# The process of converting data into vector format is called embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "9c920a65-b004-4dc3-abbc-f0fae97f7b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total number of characters:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "08386de9-8db0-40be-b7e1-476c50d61331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "ee10dd5a-be8d-4034-a923-0d51470cbdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' ', 'world,', ' ', 'this', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello world, this is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "0f68ab11-e39e-46ed-a886-a6d61e2b1543",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We refrain from making all text lowercase because capitalization helps LLMs distinguish between proper nouns and common nouns, understand sentence \n",
    "#structure, and learn to generate text with proper capitalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "e6370f3a-7e3c-44bb-b0c5-3d2161ad0cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' ', 'world', ',', '', ' ', 'this', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([, .]|\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "16aa4c26-1aeb-4985-8a4c-8eb1747a8956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', ',', 'this', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "#Result include whitespace characters, it can be excluded\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "a7879c81-22fb-4d12-b36e-f59754a5c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here one thing to be noted that, if we are building our LLM for writing a python code or any code in general, they will be sensitive to\n",
    "#indentation and spaces, in that case it becomes important to keep whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "8f917398-3e83-47ed-bf30-04f94b349ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'Is', ' ', 'this', '--', 'a', ' ', 'test', '?', '', ' ', '']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this--a test? \"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "597da59a-c3ff-4eab-9cf9-44f22f5510b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4649\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "316625dd-4e50-480d-bb0f-970911ba7b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1159\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(list(set(preprocessed)))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "ee5f0502-e48e-4d23-82ef-969a661eff3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Carlo;', 25)\n",
      "('Chicago', 26)\n",
      "('Claude', 27)\n",
      "('Come', 28)\n",
      "('Croft', 29)\n",
      "('Destroyed', 30)\n",
      "('Devonshire', 31)\n",
      "('Don', 32)\n",
      "('Dubarry', 33)\n",
      "('Emperors', 34)\n",
      "('Florence', 35)\n",
      "('For', 36)\n",
      "('Gallery', 37)\n",
      "('Gideon', 38)\n",
      "('Gisburn', 39)\n",
      "('Gisburns', 40)\n",
      "('Grafton', 41)\n",
      "('Greek', 42)\n",
      "('Grindle', 43)\n",
      "('Grindle:', 44)\n",
      "('Grindles', 45)\n",
      "('HAD', 46)\n",
      "('Had', 47)\n",
      "('Hang', 48)\n",
      "('Has', 49)\n",
      "('He', 50)\n",
      "('Her', 51)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i > 50:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "8103f8a4-922e-4652-8b4a-2582b321f7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1161\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "print(len(vocab.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "d8a3fb76-a05d-48d0-bfbb-51e47faee7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "f0832e7c-6109-40cd-b408-6d7a98a8b9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 58, 2, 872, 1013, 615, 5, 541, 763, 5, 1155, 608, 1, 69, 7, 39, 873, 1136]\n",
      "\" It' s the last, he painted, you know\" Mrs. Gisburn said with\n"
     ]
    }
   ],
   "source": [
    "#Testing the tokenizer class\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last, he painted, you know\" Mrs. Gisburn said with\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "982fcdca-2e3f-4565-a659-91a64f17d4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1156)\n",
      "('your', 1157)\n",
      "('yourself', 1158)\n",
      "('<|endoftext|>', 1159)\n",
      "('<|unk|>', 1160)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)\n",
    "#Similarly the code in simpletokenizer class is updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "d925efce-5e83-41c9-8bf3-dcce423c6f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "[1160, 5, 362, 1155, 642, 1000, 10, 1159, 57, 1013, 981, 1009, 738, 1013, 1160, 7]\n",
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "e08f7b31-5ba2-40a6-b8ce-ff665fba5677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The tokenization used was a simple one. Let's use Byte Pair Encoding(BPE). It is more sophisticated and used in GPT\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "9970b57f-fae8-42a7-8328-fefe79b5482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "1a3d62bf-d4d5-465a-bef7-58368921c8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 1295]\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of some place\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of some place\"\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "6f023aea-ea8f-485f-87e7-5107a67e8a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can make two noteworthy observations based on the token IDs and decoded text above. First, the <|endoftext|> token is assigned a relatively large \n",
    "#token ID, namely, 50256. In fact, the BPE tokenizer, which was used to train models such as GPT-2, GPT-3, and the original model used in ChatGPT, \n",
    "#has a total vocabulary size of 50,257, with <|endoftext|> being assigned the largest token ID.\n",
    "#The algorithm underlying BPE breaks down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, \n",
    "#enabling it to handle out-of-vocabulary words. So, thanks to the BPE algorithm, if the tokenizer encounters an unfamiliar word during tokenization, \n",
    "#it can represent it as a sequence of subword tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "3e9dc56b-f66f-438e-b227-18045ab5e7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text)) #After applying BPE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "3ed8a673-ff97-41b0-b09c-f3ab32e43d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:   [290, 4920, 2241, 287]\n",
      "y:        [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "#for LLM the input is text and output is the word next to it\n",
    "#consider this example\n",
    "enc_sample = enc_text[50:]\n",
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x:   {x}\")\n",
    "print(f\"y:        {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "8bed3f6d-5cb0-47b9-98c8-d3f27d601608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "source": [
    "#Here is how LLM will recieve input and the corresponding desired output\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "84618819-e5cb-45c3-b3e3-5a89e35bf0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "#Above code represent the token Id, Let's do it for the tokens\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "d2a8da32-f1b7-48d9-8757-2b8857088e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, for this project, we will handle two tensors an input and an output tensors for LLM\n",
    "#Encode method of BPE tokenizer performs both tokenization and conversion into token IDs as a single step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "298cfcf3-cd5a-4de7-8fc0-b44c4de10dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relevant libraries for pytorch\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "1b387c52-943f-4d9d-8cf8-491159a1a688",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i+max_length]\n",
    "            target_chunk = token_ids[i+1: i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "610bfeee-bd6a-409e-927f-fdb87583dc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "bb4efdf8-eaf8-463d-ace1-0fc4a8485484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ") \n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "122d668c-d59b-43ba-b053-51e3e460c669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "27f580c1-4a82-46d4-8563-8d7739d6d56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [  367,  2885,  1464,  1807],\n",
      "        [ 2885,  1464,  1807,  3619],\n",
      "        [ 1464,  1807,  3619,   402],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [  402,   271, 10899,  2138],\n",
      "        [  271, 10899,  2138,   257]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 2885,  1464,  1807,  3619],\n",
      "        [ 1464,  1807,  3619,   402],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [  402,   271, 10899,  2138],\n",
      "        [  271, 10899,  2138,   257],\n",
      "        [10899,  2138,   257,  7026]])\n"
     ]
    }
   ],
   "source": [
    "#Batch size of 1 require less memory size during training but lead to more noisy model updates.\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=1, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "d1d0cdd4-7e05-4fab-9027-04292d901888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#creating token embeddings since LLM are deep neural networks\n",
    "input_ids = torch.tensor([2,3,5,1])\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)\n",
    "#weight matrix of embedding layer contains small random values, these values are optimized during the LLM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "3235a60e-2683-4fe6-b20a-42b43d3aabcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#There is one row for each of the six possible tokens in the vocabulary. And one column for each three embedding dimensions\n",
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "37bf7a41-19ca-4059-84df-adabc5856e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "8345b33c-9fa3-426c-9c62-43e692727b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The same token id gets mapped to the same vector representation, regardless of where token id is positioned.\n",
    "#It is important to add additional position information into the LLM.\n",
    "#Positions embeddings: relative positional embeddings and absolute positional embeddings\n",
    "#More emphasis on relative position embeddings i.e. focussing on \"how far appart\" rather than \"at which exact positions\" of token ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "c5b58de0-843e-472a-b7c5-a644e574d97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Token Embeddings are suitable inputs for LLMs\n",
    "output_dim = 256\n",
    "vocab_size = 50257\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "e6c4e604-ea34-42b3-83c3-e402732b917e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "c1d2ecb8-fd00-45d8-b20e-3458653cdd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "b7da8ddc-cba9-4a51-b473-5cfd351aa8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "36176f33-84c4-40a8-9ad4-af997c060415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "a62ffdb0-7869-4b13-8dec-c872a9b33953",
   "metadata": {},
   "outputs": [],
   "source": [
    "#While token embeddings provide consistent vector representations for each token, they lack a sense of the token's position in a sequence. To rectify \n",
    "#this, two main types of positional embeddings exist: absolute and relative. OpenAI's GPT models utilize absolute positional embeddings that are added \n",
    "#to the token embedding vectors and are optimized during the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "0ab0e2c0-9ee1-4bb5-9ad2-5ac3e1b04f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The embedding weights are initialized with random values as preliminary step. These are important to convert token Ids into continous\n",
    "#vector representation, which serve as the input for LLM, which is essentially a deep neural networks. Not to forget it is trained with backpropagation algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "09ebc844-e133-4525-9a75-ac76dca5eb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor(\n",
    "[[0.43, 0.15, 0.89], # Your (x^1)\n",
    "[0.55, 0.87, 0.66], # journey (x^2)\n",
    "[0.57, 0.85, 0.64], # starts (x^3)\n",
    "[0.22, 0.58, 0.33], # with (x^4)\n",
    "[0.77, 0.25, 0.10], # one (x^5)\n",
    "[0.05, 0.80, 0.55]] # step (x^6)\n",
    ")\n",
    "\n",
    "query = inputs[1] \n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "print(attn_scores_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "a30c34cf-5198-4ef9-9526-1181afe25162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9544)\n",
      "tensor(0.9544)\n"
     ]
    }
   ],
   "source": [
    "res = 0\n",
    "for idx, element in enumerate(inputs[0]):\n",
    "    res += inputs[0][idx]*query[idx]\n",
    "print(res)\n",
    "print(torch.dot(inputs[0], query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "f92ad83b-832e-4152-923d-23f13f0ddc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The dot product is of input and query sequence, which is next element.\n",
    "#Beyond viewing the dot product operation as a mathematical tool that combines two vectors to yield a scalar value, the dot product is a measure of\n",
    "#similarity because it quantifies how much two vectors are aligned: a higher dot product indicates a greater degree of alignment or similarity between the\n",
    "#vectors. In the context of self-attention mechanisms, the dot product determines the extent to which elements in a sequence attend to each other:\n",
    "#the higher the dot product, the higher the similarity and attention score between two elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "4c40561d-8d8d-4aec-a858-4d0269adf12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "#The main goal behind the normalization shown is to obtain attention weights that sum up to 1. This normalization is a convention that is useful for \n",
    "#interpretation and for maintaining training stability in an LLM\n",
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "8f1c7281-0aa3-45e4-b90b-89e565e77759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "#Using softmax function for normalization, it ensures attention weights are always positive\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x)/torch.exp(x).sum(dim=0)\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "92caab60-0372-485e-839e-bfe43a4ebace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "#The above output are interpretable as probabilities or relative importance. Higher weights means greater importance\n",
    "#Now using the pytorch's implementation of softmax\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "bae876bc-fc8c-40ce-95a1-2c2b3f8747b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "#Now the context vector z is calculated as a weighted sum of all input vectors. It includes multiplying input vector with its corresponding attention weights.\n",
    "query = inputs[1]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "9fc6d536-5091-4f3b-88a8-1511e3f005e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "#The above attention weight was of second input element \"journey\". Now for entire sentence\n",
    "attn_scores = torch.empty(6,6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "27c167ec-d11f-4b7c-8a19-707879037d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "#Instead of using for loops, let's do it quiker and simpler way\n",
    "attn_scores = inputs@inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "69e0e0d4-cbed-4b62-98d2-7060b310fecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "#Now normalize each row such that sum of each row is equal to one.\n",
    "attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "e1ad7328-8553-4d42-826d-0f757acf5d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 2 sum: 1.0\n",
      "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "#Checking the sum of rows\n",
    "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
    "print(\"Row 2 sum:\", row_2_sum)\n",
    "print(\"All row sums:\", attn_weights.sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "d8ded40a-dd12-4ead-9eab-570a15f4f11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "#Now calculating the context vector\n",
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "e45defc2-eadb-4c2f-b4a0-a4bed364b7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For example consider only one context vector\n",
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "01c73895-2575-416f-9e17-15b20cfda8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "f1c7fdb3-f829-4aad-9fb7-417785d04239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "a037daa6-9559-4d27-996d-49295c57b0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "#Now obtaining all keys and values\n",
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "05317ab0-75aa-41a1-9ba9-7573404208cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "cc676cf7-75e0-4b5b-82b6-b3e41f2d7f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "4b51c69f-e8cf-42aa-a8be-dbb5ba4fc048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "#Now after finding attention scores, now it is time to use softmax function to find the attention weights.\n",
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2/d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "be35d052-975c-4213-a6c4-3375ae86a45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "#Now context vectors\n",
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "a772de3c-c8e9-420f-b17f-e2014ef997b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query - search query in database. It represent current item.\n",
    "#key - used for indexing and searching, used to match with query.\n",
    "#value - similar to key value pair in database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "eb1c9d67-6f4f-4460-8dfc-084a7b5192f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now organizing the above codes into a python class\n",
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "e294496d-1422-4bd1-b2e5-b959caa416ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "b66b3837-0b73-4487-832a-ba696dd8a429",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "8a99fa63-059d-40a2-86be-206234e6fbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#since nn.Linear uses more sophisticated weight initialization, the output will be different\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "d1f59233-2303-4cf1-8eee-9df1ade11bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that SelfAttention_v1 and SelfAttention_v2 give different outputs because they use different initial weights for the weight matrices since\n",
    "#nn.Linear uses a more sophisticated weight initialization scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "96ce4351-3f3c-401c-b56e-f75018e37f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Casual attention mask\n",
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "ec0ffd0b-9727-4569-aa48-ecadf503aeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "c23d3550-6c9a-49a1-be45-32bbb2acd106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Multiply with attention weights to zero the diagonals\n",
    "masked_simple = attn_weights*mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "a2677587-66b9-48f8-8359-a5fb53d9e57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Again renormalizing the attention weights\n",
    "row_sums = masked_simple.sum(dim=1, keepdim=True)\n",
    "masked_simple_norm = masked_simple/row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "2d713ba1-7404-4f67-b8ec-3669cc5b2d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#More efficient way is to mask attention scores with negative infinity values\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "d0ad66a6-2c3c-4db3-b049-99d354a6c958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Again applying softmax function\n",
    "attn_weights = torch.softmax(masked/keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "6bfcabef-f4e4-43d8-abcf-e58b65201254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2., 2., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [2., 2., 0., 0., 0., 2.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "#To prevent overfitting, randomly selected hidden layer units are ignored during training.\n",
    "#Here we will apply the dropout mask after computing the attention weights\n",
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6, 6)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "4e7bc72c-9cad-44ad-9409-9bdb97519616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.8966, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4921, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4350, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#The above matrix is scaled, since 50% of values are reduced to zero, rest 50% are increased to 2.\n",
    "#Now applying dropout to our attention weight matrix\n",
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "b55b1915-168c-463c-941c-cb3a7519ba53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "3bf786e5-04be-4964-83d4-e6c525299d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "861eaf35-96af-4f1d-9791-c74426fc77a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CasualAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "86c6c5af-2380-4add-95d4-81d8dfb05a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create a list of CausalAttention heads\n",
    "        self.heads = nn.ModuleList([\n",
    "            CasualAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply each attention head to the input and concatenate the outputs\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "9bc9a97f-026f-4669-b727-2e5e52282000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0671,  0.3759,  0.1906,  0.0834, -0.2695,  0.0855,  0.1404,\n",
      "           0.0812],\n",
      "         [-0.0255, -0.1609, -0.0815, -0.0234,  0.1689, -0.0391, -0.0457,\n",
      "          -0.0262],\n",
      "         [ 0.0591, -0.1720, -0.0297, -0.0817,  0.2631, -0.1513, -0.0832,\n",
      "           0.0431],\n",
      "         [-0.0906, -0.2119, -0.1133, -0.1110, -0.0358, -0.0650, -0.1195,\n",
      "          -0.1228],\n",
      "         [-0.2134, -0.5350, -0.2792,  0.0115,  0.2884,  0.0613, -0.1142,\n",
      "          -0.1195]],\n",
      "\n",
      "        [[ 0.5806, -0.1113,  0.3619, -1.0039,  0.3026, -1.2578, -0.4240,\n",
      "           0.4896],\n",
      "         [-0.1215, -0.9013, -0.3617, -0.7995,  0.3469, -0.7924, -0.5056,\n",
      "           0.0874],\n",
      "         [-0.3242, -0.9644, -0.6044, -0.4799,  0.3213, -0.4401, -0.4670,\n",
      "          -0.0200],\n",
      "         [-0.2342, -0.8020, -0.5235, -0.1915,  0.3237, -0.2161, -0.3542,\n",
      "          -0.1938],\n",
      "         [-0.3371, -0.7308, -0.5518, -0.1269,  0.2574, -0.0218, -0.2757,\n",
      "          -0.3226]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# Example batch input tensor (batch_size=2, num_tokens=5, d_in=3)\n",
    "batch = torch.randn(2, 5, 3)\n",
    "\n",
    "# Extract context length from the batch (number of tokens)\n",
    "context_length = batch.shape[1]\n",
    "\n",
    "# Input and output dimensions for each attention head\n",
    "d_in, d_out = 3, 2\n",
    "\n",
    "# Define number of attention heads\n",
    "num_heads = 4\n",
    "\n",
    "# Initialize the MultiHeadAttentionWrapper with dropout set to 0.0\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, dropout=0.0, num_heads=num_heads)\n",
    "\n",
    "# Pass the batch through the multi-head attention\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "# Print the output context vectors and their shape\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "8043dff5-d1b2-4f22-aa29-33ad663d26fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining the MultiHeadAttentionWraper and CausalAttention into single class\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out//num_heads\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer(\n",
    "            'mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(-2,-1)\n",
    "\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, float('-inf'))\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / (self.head_dim**0.5), dim=-1)\n",
    "\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = attn_weights @ values\n",
    "\n",
    "        context_vec = context_vec.transpose(1, 2)\n",
    "\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "b0425fbb-42a7-447e-880d-d08d24e84232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Vectors:\n",
      " tensor([[[0.3534, 0.2698],\n",
      "         [0.4384, 0.3512],\n",
      "         [0.4285, 0.3398],\n",
      "         [0.4114, 0.3161],\n",
      "         [0.4165, 0.3216]],\n",
      "\n",
      "        [[0.2463, 0.2878],\n",
      "         [0.3035, 0.2797],\n",
      "         [0.2544, 0.2600],\n",
      "         [0.2794, 0.3025],\n",
      "         [0.2991, 0.2918]],\n",
      "\n",
      "        [[0.2466, 0.1752],\n",
      "         [0.2942, 0.2428],\n",
      "         [0.3016, 0.3003],\n",
      "         [0.3301, 0.3370],\n",
      "         [0.3675, 0.3662]]], grad_fn=<ViewBackward0>)\n",
      "Context Vectors Shape: torch.Size([3, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_size = 3\n",
    "context_length = 5\n",
    "d_in = 4\n",
    "batch = torch.rand(batch_size, context_length, d_in)\n",
    "d_out = 2\n",
    "num_heads = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, dropout=0.0, num_heads=num_heads)\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(\"Context Vectors:\\n\", context_vecs)\n",
    "print(\"Context Vectors Shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "f6b3fcce-6979-4bf1-babb-ca27f24bc033",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "\"vocab_size\": 50257, # Vocabulary size\n",
    "\"context_length\": 1024, # Context length\n",
    "\"emb_dim\": 768, # Embedding dimension\n",
    "\"n_heads\": 12, # Number of attention heads\n",
    "\"n_layers\": 12, # Number of layers\n",
    "\"drop_rate\": 0.1, # Dropout rate\n",
    "\"qkv_bias\": False # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "1043964e-8cf3-4cd6-b6e9-87d83573cf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "\n",
    "        x = self.trf_blocks(x)\n",
    "\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "e90d750a-7661-48f8-82c0-9d2efffed851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "21af922f-a6b1-4bd2-bbea-c6ff2ea79cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n",
      "         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n",
      "         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n",
      "         [ 0.0447,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n",
      "\n",
      "        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n",
      "         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],\n",
      "         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],\n",
      "         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "a8bc5737-3e70-4899-aa90-43ebbeaf8540",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self,x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "5f879fe0-c05f-4dfa-b572-0efdcacf40c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GELU activation function\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "f6d60af4-cee5-4273-bb26-9558f5036146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define GELU activation function\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + torch.tanh(\n",
    "        torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "        (x + 0.044715 * torch.pow(x, 3))\n",
    "    ))\n",
    "\n",
    "# Use PyTorch's built-in ReLU\n",
    "def relu(x):\n",
    "    return F.relu(x)\n",
    "\n",
    "# Input values\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "\n",
    "# Compute the activations\n",
    "y_gelu = gelu(x)\n",
    "y_relu = relu(x)\n",
    "\n",
    "# Plot the functions\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x.numpy(), y.numpy())  # Convert tensors to numpy arrays for plotting\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "d2a03e8b-5abd-491d-ad31-79cc81c7881e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4*cfg[\"emb_dim\"]),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "ef0c786c-4954-4951-8178-0b15b80aedba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "b464683a-7f62-4810-a5e1-5480fc5692b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut=False):\n",
    "        super(ExampleDeepNeuralNetwork, self).__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[i], layer_sizes[i+1]), nn.GELU())\n",
    "            for i in range(len(layer_sizes) - 1)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x)\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "114ce458-387b-4e56-bdd8-618833cfb37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output without shortcut: tensor([[0.0610]], grad_fn=<GeluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=False)\n",
    "\n",
    "output_without_shortcut = model_without_shortcut(sample_input)\n",
    "\n",
    "print(\"Model output without shortcut:\", output_without_shortcut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "aa19c145-d68c-4e66-ab09-fb95a082bd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradients(model, x):\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    loss = loss_fn(output, target)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "498aa46c-802c-4a58-82cb-3d4912c4e9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.0002017411752603948\n",
      "layers.1.0.weight has gradient mean of 0.00012011770741082728\n",
      "layers.2.0.weight has gradient mean of 0.0007152437465265393\n",
      "layers.3.0.weight has gradient mean of 0.0013988513965159655\n",
      "layers.4.0.weight has gradient mean of 0.005049604922533035\n"
     ]
    }
   ],
   "source": [
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "471cf262-5cc3-46fa-afcf-825e07eacb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22186797857284546\n",
      "layers.1.0.weight has gradient mean of 0.207092747092247\n",
      "layers.2.0.weight has gradient mean of 0.32923877239227295\n",
      "layers.3.0.weight has gradient mean of 0.2667771875858307\n",
      "layers.4.0.weight has gradient mean of 1.3268063068389893\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=True\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "279a742a-1ebf-4339-84d0-5e6fe0edbd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "7f60fee4-3160-4260-be60-65a7a63d9d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layer normalization (LayerNorm) is applied before each of these two components, and dropout is applied after them to regularize the model and\n",
    "#prevent overfitting. This is also known as Pre-LayerNorm. Older architectures, such as the original transformer model, applied layer\n",
    "#normalization after the self-attention and feed-forward networks instead, known as Post-LayerNorm, which often leads to worse training dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "b41ce301-65b3-4fd9-80d8-912ddbee4f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "#Feeding some sample data to transformer block\n",
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "cf342fff-5243-4f74-913d-89560cdf38a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "\n",
    "        x = self.trf_blocks(x)\n",
    "\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        logits = self.out_head(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "06ad623b-661c-42ad-a775-b3086b75d221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.1380,  0.0077, -0.1964,  ..., -0.0223, -0.1060,  0.1716],\n",
      "         [ 0.3864, -0.8408, -0.6565,  ..., -0.5163,  0.2369, -0.3358],\n",
      "         [ 0.6988, -0.1828, -0.1630,  ...,  0.1472, -0.6504, -0.0057],\n",
      "         [-0.4290,  0.1669, -0.1258,  ...,  1.1579,  0.5303, -0.5549]],\n",
      "\n",
      "        [[ 0.1093, -0.2894, -0.1467,  ..., -0.0558,  0.2910, -0.2824],\n",
      "         [ 0.0882, -0.3552, -0.3526,  ...,  1.2929,  0.0053,  0.1897],\n",
      "         [ 0.6091,  0.4702, -0.4094,  ...,  0.7688,  0.3788, -0.1974],\n",
      "         [-0.0613, -0.0736,  0.4750,  ...,  1.2462, -0.3835,  0.0608]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "8e5452a4-dd29-47d9-9e47-07a9b9042b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input tensors have ids of two text input with four tokens and output has shape of [2, 4, 50257] Here 50257 is vocabulary size of tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "1c0e614e-fb1d-4d5b-b7e6-c11702013382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "b814a63d-c712-4c02-a36d-554180e34d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Why is there 163 million parameter, when GPT_2 has 124 million?\n",
    "#The reason is a concept called weight tying that is used in the original GPT-2 architecture, which means that the original GPT-2 architecture is reusing the\n",
    "#weights from the token embedding layer inits output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "d4002cec-98c3-4e1e-9847-28ef08847d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "bfa9d158-6fd7-4eac-af63-078f67e51386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 124412160\n"
     ]
    }
   ],
   "source": [
    "#The token embedding and output layers are very large due to the number of rows for the 50,257 in the tokenizer's vocabulary. Removing output layer parameters from GPT-2 model count\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params_gpt2 = total_params - sum(p.numel() for p in model.out_head.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "e47b2a3b-ecbb-4c19-9c8e-06fa6545c7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "#Now the number of parameters is matching the total size of GPT-2 model. But weight tying is necessary to reduce the overall memory footprint and  computational complexity of the model.\n",
    "#Now computing the memory requirements for 163 million parameters.\n",
    "total_size_bytes = total_params*4\n",
    "total_size_mb = total_size_bytes / (1024*1024)\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "aed7abac-28ce-4e5c-b6da-0756505d6b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Even a small LLM require 621 MB. Here each parameter is a 32-bit float taking up 4 bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "0d08a8c2-c440-4d9b-9b37-88cdfcb7da09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now converting the output tensors back to text. It requires decoding the output tensors, selecting token based on probability distribution\n",
    "#and converting these tokens into human-readable text. The vector corresponding to the next token is extracted and converted into a probability \n",
    "#distribution via softmax function. Within the vector containing the resulting probability scores, the index of the highest\n",
    "#value is located, which translates to the token ID. This token ID is then decoded back into text, producing the next token in the sequence. Finally, this\n",
    "#token is appended to the previous inputs, forming a new input sequence for the subsequent iteration. This step-by-step process enables the model to\n",
    "#generate text sequentially, building coherent phrases and sentences from the initial input context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "01a6f426-c620-4284-8a6d-2774f88ff8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Token generating process\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "2a1022de-fe60-4fb1-b50c-feada27e0fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the preceeding code, the generate_text_simple function, we use a softmax function to convert the logits into a probability distribution from\n",
    "#which we identify the position with the highest value via torch.argmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "75464be2-5ab2-4c95-ac1c-df2bc88c0ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "9774f936-bcf7-4d0b-84ee-e6f68d7bd15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor,\n",
    "    max_new_tokens=6,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    ")\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "69c7c90a-2969-4ace-ac7f-f5592ac55a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "f3490e84-7ec8-4f1d-a2ee-6c9cb34a1f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model produced gibberish. It didn't produced coherent text because it was not trained. We just implemented GPT architecture\n",
    "#with initial random weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "5a66a26f-5d95-4454-9718-e5621b881fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "\"vocab_size\": 50257, \n",
    "\"context_length\": 256, \n",
    "\"emb_dim\": 768, \n",
    "\"n_heads\": 12, \n",
    "\"n_layers\": 12, \n",
    "\"drop_rate\": 0.1, \n",
    "\"qkv_bias\": False \n",
    "}\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "a8ee2f52-4004-433b-9c24-670cfd84ae72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "#Reduced the context length to 256 tokens. It reduces the computational demand of training model, allowing it to be trained on the standard laptop.\n",
    "import tiktoken\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "cc4c3104-c8b7-4234-9a72-d74037069823",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple example\n",
    "inputs = torch.tensor([[16833, 3626, 6100], [40, 1107, 588]])\n",
    "targets = torch.tensor([[3626, 6100, 345], [588, 428, 11311]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "8cc20917-ca69-4527-a142-03fd018c031c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(probas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "4f52711d-82da-40fc-b7d3-cae8b155fa90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "#Applying argmax to probability scores\n",
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "3faef27d-681e-470c-afc2-d3c64f4fd955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "f669c2b0-ba2f-4865-90a1-915197aab5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4536e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([3.9834e-05, 1.6783e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "#Text is generated but the model is not trained. Thats why undesired result.\n",
    "#Since the vocabulary we are using contains 50,257 tokens, therefore initial probabilities hover around 1/50257 i.e 0.00002\n",
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx, [0, 1, 2]]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx, [0, 1, 2]]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "8accd439-be6d-476b-9fed-6dd38efd6493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log probabilities: tensor([ -9.5042, -10.3796, -11.3677, -10.1308, -10.9951, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "#The goal of training an LLM is to maximize these values, aiming to get them as close to a probability of 1. This way, we ensure the LLM consistently\n",
    "#picks the target token—essentially the next word in the sentence—as the next token it generates.\n",
    "#Updation of weights is done via backpropagation.\n",
    "#Applying log of probabilities\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(\"Log probabilities:\", log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "6487c0a7-5a41-47c2-b9e5-2f5b58db3b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7723)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "8b20d8b9-cf3c-47c5-b749-970476bedf17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7723)\n"
     ]
    }
   ],
   "source": [
    "# The goal is to get the average log probability as close to 0 as possible by updating the model's weights as part of the training process\n",
    "#However, in deep learning, the common practice isn't to push the average log probability up to 0 but rather to bring the negative average log probability\n",
    "#down to 0. The negative average log probability is simply the average log probability multiplied by -1\n",
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "a5d9bbce-a36d-4c9e-bc62-4312c9931705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "#This value is known as cross entropy loss.\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "5af372a0-82be-463b-95a7-f64830677848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "#Flatten these tensors\n",
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "2b4b484d-8df5-425c-bb15-02f43b10d013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: tensor(10.7723)\n"
     ]
    }
   ],
   "source": [
    "#Earlier we applied softmax function, selected the probability scores and computed the negative average log probabilities. \n",
    "#Py-torch cross-entropy function will take care of all these.\n",
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(\"Cross-entropy loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "a0546cdf-256b-46b3-b230-2b364ba653b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "2b389635-30b6-48c4-9769-7e5d697f4927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "1e744ab6-766d-4ef6-8bc0-d4eb039bec51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#90% data for training and 10% for validation\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "48ce4e01-9e8e-40d7-812f-564459f68eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "484a1514-9686-4f95-903b-e8d425a2ddc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x,y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x,y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "17554f05-d806-446a-a253-19e3c63c929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0,1), target_batch.flatten()\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "62284c51-c1fc-44de-a908-1ce610547016",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing training and validation loss\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.0\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss/num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "872e221f-ec04-4530-add2-a4e61a1f25b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.98758210076226\n",
      "Validation loss: 10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "train_loss = calc_loss_loader(train_loader, model, device)\n",
    "val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "52b813a1-0a30-4e28-b835-4f448319ea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The loss value is high, since the model is not trained, as it trains, the loss value approaches 0.\n",
    "def train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "    eval_freq, eval_iter, start_context\n",
    "):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(\n",
    "                    f\"Ep {epoch + 1} (Step {global_step:06d}): \"\n",
    "                    f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\"\n",
    "                )\n",
    "        generate_and_print_sample(\n",
    "            model, train_loader.dataset.tokenizer, device, start_context\n",
    "        )\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "740553d2-1c16-466e-9c90-a46f53e84870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        val_loss = calc_loss_loader(\n",
    "            val_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "a94bfd89-00a6-429b-8bef-00bb420ba85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model,\n",
    "            idx=encoded,\n",
    "            max_new_tokens=50,\n",
    "            context_size=context_size\n",
    "        )\n",
    "\n",
    "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "\n",
    "        print(decoded_text.replace(\"\\n\", \" \"))\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "e95d9b6d-cb27-405d-bf29-d7bc4841f832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.830, Val loss 9.927\n",
      "Ep 1 (Step 000005): Train loss 8.133, Val loss 8.334\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010): Train loss 6.770, Val loss 7.047\n",
      "Ep 2 (Step 000015): Train loss 6.497, Val loss 6.573\n",
      "Every effort moves you, and,, and, and,,,,, and, and,,,,,,,,,,,,,, and,,,, and,, and,,,,, and,,,,,,\n",
      "Ep 3 (Step 000020): Train loss 5.578, Val loss 6.491\n",
      "Ep 3 (Step 000025): Train loss 4.731, Val loss 6.386\n",
      "Every effort moves you, and to the picture.                      \"I, and the of the of the's the honour, and, and I had been, and I\n",
      "Ep 4 (Step 000030): Train loss 5.281, Val loss 6.361\n",
      "Ep 4 (Step 000035): Train loss 3.851, Val loss 6.261\n",
      "Every effort moves you of the to the picture to the of the picture--and a of the picture.         \"I was the picture--as Jack's the his pictures--and it the picture and I was his I was his\n",
      "Ep 5 (Step 000040): Train loss 3.658, Val loss 6.198\n",
      "Every effort moves you know the \"Oh, and he was not the fact by his last word.         \"I was.      \"Oh, I felt a little a little the    \n",
      "Ep 6 (Step 000045): Train loss 3.588, Val loss 6.140\n",
      "Ep 6 (Step 000050): Train loss 2.370, Val loss 6.112\n",
      "Every effort moves you know; and my dear, and he was not the fact with a little of the house of the fact of the fact, and.                       \n",
      "Ep 7 (Step 000055): Train loss 2.321, Val loss 6.139\n",
      "Ep 7 (Step 000060): Train loss 2.680, Val loss 6.181\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"I looked--as of the fact, and I felt him--his back his head to the donkey. \"Oh, and_--because he had always _\n",
      "Ep 8 (Step 000065): Train loss 1.578, Val loss 6.180\n",
      "Ep 8 (Step 000070): Train loss 1.326, Val loss 6.182\n",
      "Every effort moves you?\" \"I didn't bear the picture--I told me.  \"I looked up, and went on groping and Mrs. I was back the head to look up at the honour being _mine_--because he was when I\n",
      "Ep 9 (Step 000075): Train loss 1.080, Val loss 6.282\n",
      "Ep 9 (Step 000080): Train loss 0.655, Val loss 6.287\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 10 (Step 000085): Train loss 0.516, Val loss 6.334\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to the donkey again. I saw that, when Stroud laid in the first\n"
     ]
    }
   ],
   "source": [
    "#We are using AdamW optimizer, it minimize model complexity and prevents overfitting by penalizing larger weights.\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.01)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=num_epochs,\n",
    "    eval_freq=5,\n",
    "    eval_iter=1,\n",
    "    start_context=\"Every effort moves you\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "2042b300-e7c7-4578-950f-55cede14127e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses)).numpy()\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "1708cc38-30a7-42d6-82b7-95bc2a5124b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This divergence and the fact that the validation loss is much larger than the training loss indicate that the model is overfitting to the training data. We can\n",
    "#confirm that the model memorizes the training data verbatim by searching for the generated text snippets, such as \"quite insensible to the irony\" in\n",
    "#the \"The Verdict\" text file. This memorization is expected since we are working with a very, very small\n",
    "#training dataset and training the model for multiple epochs. Usually, it's common to train a model on a much, much larger dataset for only one epoch.\n",
    "model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "5dfec1ce-3b7b-43d4-b0c7-238436fb742d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "e80a7c16-d290-40be-9a4f-3a3833ffed72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature, top_k=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float('-inf')).to(logits.device),\n",
    "                logits\n",
    "            )\n",
    "\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "ad89e01b-621b-40e5-a54a-acfb1a70db45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you know began to my surprise, a little it was the\n",
      "\"Ah enough\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "3b2e3cca-65a6-4d5c-8f7d-a0386e88a8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The generated text is different from previous one where the model memorized it.\n",
    "#Saving the PyTorch model\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "0d688037-0b3c-49fc-b6bd-30ca5321b3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_27940\\4111636023.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 578,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the model weights into the new GPTModel\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "f6f07381-c379-40e5-b310-f459538cfd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive optimizers such as AdamW store additional parameters for each model weight. AdamW uses historical data to adjust learning rates for each\n",
    "#model parameter dynamically. Without it, the optimizer resets, and the model may learn suboptimally or even fail to converge properly, which means that it\n",
    "#will lose the ability to generate coherent text. . Using torch.save, we can save both the model and optimizer state_dict contents as follows\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "},\n",
    "          \"model_and_optimizer.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "0784da4e-83a9-402e-bf7e-44a9bb0d46bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_27940\\4016816238.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"model_and_optimizer.pth\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 582,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\")\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "0e90bba2-1621-4ab8-8b06-7f36e67da58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fortunately, OpenAI openly shared the weights of their GPT-2 models, thus eliminating the need to invest tens to hundreds of thousands of dollars in\n",
    "#retraining the model on a large corpus ourselves.\n",
    "#OpenAI saved the weights via TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "69b8b4f4-5305-4aea-b2fc-25ead66fb786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "099c6a5a-7b72-4fc5-afb7-a4ebd72524d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt_download.py', <http.client.HTTPMessage at 0x208dc457a40>)"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = (\n",
    " \"https://raw.githubusercontent.com/rasbt/\"\n",
    " \"LLMs-from-scratch/main/ch05/\"\n",
    " \"01_main-chapter-code/gpt_download.py\"\n",
    " )\n",
    "filename = url.split('/')[-1]\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "0b7c18fa-5497-4057-be41-bd92e14176c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 70.9kiB/s]\n",
      "encoder.json: 100%|██████████| 1.04M/1.04M [00:02<00:00, 369kiB/s] \n",
      "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 47.1kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [08:56<00:00, 928kiB/s]    \n",
      "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<?, ?iB/s]\n",
      "model.ckpt.meta: 100%|██████████| 471k/471k [00:02<00:00, 229kiB/s]  \n",
      "vocab.bpe: 100%|██████████| 456k/456k [00:02<00:00, 203kiB/s]  \n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "\n",
    "model_size = \"124M\"\n",
    "models_dir = \"./models\"\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "0ec8a315-5102-4b29-8e19-d057d05f2212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings:\", settings)\n",
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "c7d1a561-a42e-47fe-80f5-69f82608868f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embedding weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "print(params[\"wte\"])\n",
    "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "id": "8ac4ca29-3777-4ae8-b1ce-25f160e0d463",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    " \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    " \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    " \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    " \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "5688b41b-6402-4f52-852f-d16b0f359dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 600,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"gpt2-small (124M)\"\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024})\n",
    "NEW_CONFIG.update({\"qkv_bias\": True})\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "3418ddfc-b071-4ad3-86f9-0d62e7e64ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "4ceb2a10-0722-4309-a6b2-da4a268095d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the weights\n",
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])  # A\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "\n",
    "    for b in range(len(params[\"blocks\"])):  # B\n",
    "        # Split the 'w' key for query, key, value weights\n",
    "        q_w, k_w, v_w = np.split(params[\"blocks\"][b][\"attn\"][\"c_attn\"][\"w\"], 3, axis=-1)  # C\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        # Split the 'b' key for query, key, value biases\n",
    "        q_b, k_b, v_b = np.split(params[\"blocks\"][b][\"attn\"][\"c_attn\"][\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        # Assign weights and biases for the attention output projection\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(gpt.trf_blocks[b].att.out_proj.weight, params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(gpt.trf_blocks[b].att.out_proj.bias, params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        # Assign weights and biases for the feed-forward layers\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(gpt.trf_blocks[b].ff.layers[0].weight, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(gpt.trf_blocks[b].ff.layers[0].bias, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(gpt.trf_blocks[b].ff.layers[2].weight, params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(gpt.trf_blocks[b].ff.layers[2].bias, params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        # Assign layer normalization weights and biases\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(gpt.trf_blocks[b].norm1.scale, params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(gpt.trf_blocks[b].norm1.shift, params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(gpt.trf_blocks[b].norm2.scale, params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(gpt.trf_blocks[b].norm2.shift, params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    # Assign weights and biases for the final normalization\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "\n",
    "    # Assign weights for the output head\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "\n",
    "    return gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "aca167c1-f7b8-4f9c-bf59-4c5f717fc1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 610,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "d6ed2c1c-a33f-46c9-bb2d-ee04b5e03a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you toward an equal share for each vote plus half. Inequality is often not an accurate representation of human worth; to know the\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "model=gpt,\n",
    "idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "max_new_tokens=25,\n",
    "context_size=NEW_CONFIG[\"context_length\"],\n",
    "top_k=50,\n",
    "temperature=1.5\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "id": "228462b8-fa86-4ac3-a8a3-73b2938991fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model reproduced the coherent text. Hence model weights were loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87509823-1532-4984-8dd6-66e8914a9aee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
