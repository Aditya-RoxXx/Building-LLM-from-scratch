# Building-LLM-from-scratch
Developed a Large Language Model (LLM) from the ground up following the principles and techniques from the book titled "LLM from Scratch."
Implemented key components such as tokenization, embedding layers, attention mechanisms, transformer blocks, and output decoding to build a complete language model architecture.
Optimized model training using custom training loops, data batching, and learning rate schedules, ensuring efficient convergence.
Integrated various techniques for natural language understanding and generation, focusing on scaling up the model with multiple transformer layers and attention heads.
Gained in-depth understanding of the transformer architecture, including positional encoding, multi-head attention, and feed-forward networks.
Also utilized OpenAI GPT-2 weights to initialize model parameters, leveraging pre-trained embeddings and weights to enhance training efficiency and improve the model's initial performance.
